name: Build llama.cpp for RTXâ€¯5090

on:
  workflow_dispatch:
  push:
    branches: [main]

jobs:
  build:
    runs-on: windows-2022

    steps:
    - uses: actions/checkout@v4
    - name: Setup CUDA 12.8
      uses: ./.github/actions/windows-setup-cuda
      with:
        cuda_version: '12.8'

    - name: Configure
      shell: pwsh
      run: |
        cmake -B build -G "Ninja" `
          -DCMAKE_BUILD_TYPE=Release `
          -DLLAMA_CUDA=ON -DLLAMA_CUBLAS=ON `
          -DCMAKE_CUDA_ARCHITECTURES=120 `
          -DLLAMA_BUILD_SERVER=ON

    - name: Build
      shell: pwsh
      run: cmake --build build --parallel $env:NUMBER_OF_PROCESSORS

    - uses: actions/upload-artifact@v4
      with:
        name: llama-win-sm120
        path: build/bin/*
