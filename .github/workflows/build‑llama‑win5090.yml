name: Build llama.cpp for RTXâ€¯5090

on:
  workflow_dispatch:
  push:
    branches: [main]

jobs:
  build:
    runs-on: windows-2022

    steps:
    - uses: actions/checkout@v4
    - name: Setup CUDA 12.8
      uses: ./.github/actions/windows-setup-cuda
      with:
        cuda_version: '12.8'

    - name: Build
      shell: cmd
      run: |
        call "%ProgramFiles%\Microsoft Visual Studio\2022\Enterprise\VC\Auxiliary\Build\vcvars64.bat"
        cmake -S . -B build -G "Ninja" ^
          -DLLAMA_BUILD_SERVER=ON ^
          -DCMAKE_BUILD_TYPE=Release ^
          -DGGML_CUDA=ON ^
          -DLLAMA_CURL=OFF ^
          -DGGML_STATIC=ON ^
          -DCMAKE_CUDA_ARCHITECTURES=120
        set /A NINJA_JOBS=%NUMBER_OF_PROCESSORS%-1
        cmake --build build -j %NINJA_JOBS% --config Release


    - uses: actions/upload-artifact@v4
      with:
        name: llama-win-sm120
        path: build/bin/*